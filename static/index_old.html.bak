<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Voice-Assistant with OpenAI</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <style>
    body { font-family: system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif; padding: 20px; line-height: 1.4; }
    button { padding: 10px 16px; font-size: 16px; margin-right: 8px; }
    #status { margin-top: 10px; white-space: pre-wrap; }
    .muted { color: #666; font-size: 14px; }
  </style>
</head>
<body>
  <h2>Voice to Voice Assistant</h2>
  <p class="muted">Records mic â†’ POST <code>/chat</code> â†’ plays MP3. No calls to <code>/v1/*</code>.</p>

  <button id="record">Start Recording</button>
  <button id="stop" disabled>Stop</button>
  <div id="status"></div>
  <audio id="response-audio" controls></audio>

  <hr />
  <h3>Live Stream via WebRTC with Voice Detection</h3>
  <p class="muted"><strong>How to use:</strong> Click "Start Voice Chat", then speak clearly and loudly. Wait 3 seconds of silence after finishing. The system requires at least 2.5 seconds of clear speech.</p>
  <p class="muted"><strong>Tips:</strong> Speak directly into your microphone. Avoid background noise. Speak louder than normal conversation.</p>
  <button id="live-start">Start Voice Chat</button>
  <button id="live-stop" disabled>Stop Voice Chat</button>
  <div id="live-status" class="muted"></div>
  <div id="voice-indicator" style="margin-top: 10px; padding: 10px; border-radius: 5px; display: none;">
    <div id="listening-indicator">ðŸŽ¤ Listening for your voice...</div>
    <div id="speaking-indicator" style="display: none;">ðŸ”´ Processing your message...</div>
    <div id="replying-indicator" style="display: none;">ðŸ”Š AI is responding...</div>
  </div>

  <script>
    // --- Guard: scream if anything tries to call /v1/* from this page ---
    const _origFetch = window.fetch;
    window.fetch = async (input, init) => {
      const url = typeof input === 'string' ? input : input.url;
      if (url && (url.startsWith('/v1/') || url.includes('/v1/'))) {
        console.error('Blocked a /v1/* call from this page:', url);
        throw new Error('This page must not call ' + url);
      }
      return _origFetch(input, init);
    };
    
    // Also block XMLHttpRequest calls to /v1/*
    const _origXHR = window.XMLHttpRequest.prototype.open;
    window.XMLHttpRequest.prototype.open = function(method, url, ...args) {
      if (url && (url.startsWith('/v1/') || url.includes('/v1/'))) {
        console.error('Blocked XHR /v1/* call from this page:', url);
        throw new Error('This page must not call ' + url);
      }
      return _origXHR.call(this, method, url, ...args);
    };

    let recorder, audioStream;
    let chunks = [];
    const recordBtn = document.getElementById('record');
    const stopBtn = document.getElementById('stop');
    const statusDiv = document.getElementById('status');
    const responseAudio = document.getElementById('response-audio');

    function setStatus(msg) {
      statusDiv.textContent = msg;
      console.log(msg);
    }

    async function getRecorder() {
      if (!navigator.mediaDevices?.getUserMedia) {
        throw new Error('getUserMedia not supported in this browser.');
      }
      const stream = await navigator.mediaDevices.getUserMedia({ audio: { echoCancellation: true, noiseSuppression: true, autoGainControl: true, channelCount: 1 } });

      // Prefer an explicit mime type if supported
      const preferred = 'audio/webm;codecs=opus';
      const type = MediaRecorder.isTypeSupported?.(preferred) ? preferred : 'audio/webm';
      const rec = new MediaRecorder(stream, { mimeType: type });

      return { rec, stream, type };
    }

    recordBtn.onclick = async () => {
      try {
        const { rec, stream, type } = await getRecorder();
        recorder = rec;
        audioStream = stream;
        chunks = [];
        setStatus('Recording... click "Stop" when done.\nFormat: ' + type);

        recorder.ondataavailable = (e) => { if (e.data && e.data.size) chunks.push(e.data); };
        recorder.onstop = async () => {
          try {
            setStatus('Processing...');
            const blob = new Blob(chunks, { type }); // collected data is stored as webm/opus format
            chunks = [];

            const form = new FormData();
            // Backend accepts webm/opus just fine; filename helps model inference.
            // Appended as a form data
            form.append('file', blob, 'input.webm');

            const res = await fetch('/chat', { method: 'POST', body: form });

            if (!res.ok) {
              // Try to read error detail from FastAPI
              let detail = '';
              try { detail = await res.text(); } catch {}
              throw new Error('Server error ' + res.status + (detail ? (': ' + detail) : ''));
            }

            const buf = await res.arrayBuffer();   // streaming â†’ full blob is fine for now
            const outBlob = new Blob([buf], { type: 'audio/mpeg' });
            const url = URL.createObjectURL(outBlob);

            responseAudio.src = url;
            await responseAudio.play().catch(() => {/* autoplay may be blocked */});
            setStatus('Response ready!');

          } catch (err) {
            console.error(err);
            setStatus('Error: ' + err.message);
          }
        };

        recorder.start(); // start chunks
        recordBtn.disabled = true;
        stopBtn.disabled = false;

      } catch (err) {
        console.error(err);
        setStatus('Mic error: ' + err.message);
      }
    };

    stopBtn.onclick = () => {
      try {
        if (recorder && recorder.state !== 'inactive') {
          recorder.stop();
        }
        if (audioStream) {
          audioStream.getTracks().forEach(t => t.stop());
        }
      } finally {
        recordBtn.disabled = false;
        stopBtn.disabled = true;
      }
    };
  // WebRTC Live Audio Section
  const liveStart = document.getElementById('live-start');
  const liveStop = document.getElementById('live-stop');
  const liveStatus = document.getElementById('live-status');
  const voiceIndicator = document.getElementById('voice-indicator');
  const listeningIndicator = document.getElementById('listening-indicator');
  const speakingIndicator = document.getElementById('speaking-indicator');
  const replyingIndicator = document.getElementById('replying-indicator');
  let liveStream = null;
  let pc = null;
  let ws = null;

  function setLiveStatus(msg) {
    liveStatus.textContent = msg;
    console.log('[LIVE]', msg);
  }
  
  function showVoiceState(state) {
    // Hide all indicators first
    listeningIndicator.style.display = 'none';
    speakingIndicator.style.display = 'none';
    replyingIndicator.style.display = 'none';
    
    // Show the current state
    switch(state) {
      case 'listening':
        voiceIndicator.style.display = 'block';
        voiceIndicator.style.backgroundColor = '#e8f5e8';
        listeningIndicator.style.display = 'block';
        break;
      case 'processing':
        voiceIndicator.style.display = 'block';
        voiceIndicator.style.backgroundColor = '#fff3cd';
        speakingIndicator.style.display = 'block';
        break;
      case 'replying':
        voiceIndicator.style.display = 'block';
        voiceIndicator.style.backgroundColor = '#d1ecf1';
        replyingIndicator.style.display = 'block';
        break;
      case 'hidden':
        voiceIndicator.style.display = 'none';
        break;
    }
  }
  // Websocket is opened when clicked Start live web RTC
  liveStart.onclick = async () => {
    try {
      // Peer connection var is set
      pc = new RTCPeerConnection();
      ws = new WebSocket(
        (location.protocol === 'https:' ? 'wss://' : 'ws://') + location.host + '/ws'
      );
      setLiveStatus('Connecting WebSocket for signalingâ€¦');
      await new Promise((resolve, reject) => {
        ws.onopen = resolve;
        ws.onerror = () => reject(new Error('WebSocket failed to open'));
      });

      setLiveStatus('Getting microphoneâ€¦');
      // The microphone access is requested with echo cancellation and noise suppression
      liveStream = await navigator.mediaDevices.getUserMedia({ 
        audio: { 
          echoCancellation: true, 
          noiseSuppression: true, 
          autoGainControl: true,
          channelCount: 1
        } 
      });
      // Getting audio tracks
      liveStream.getTracks().forEach(track => pc.addTrack(track, liveStream));

      pc.onicecandidate = (ev) => {
        if (ev.candidate && ws && ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify({ type: 'candidate', candidate: ev.candidate }));
        }
      };
      // Listen for any messages
      ws.onmessage = async (event) => {
        let audioBlob = null;
        // Try to handle binary audio data in the most robust way
        if (event.data instanceof Blob) {
          audioBlob = event.data;
        } else if (event.data instanceof ArrayBuffer) {
          audioBlob = new Blob([event.data], { type: 'audio/mpeg' });
        } else if (event.data && event.data.arrayBuffer) {
          // For some browsers, event.data may look like a Blob but not evaluate instanceof
          try {
            const buffer = await event.data.arrayBuffer();
            audioBlob = new Blob([buffer], { type: 'audio/mpeg' });
          } catch {}
        }

        if (audioBlob) {
          showVoiceState('replying');
          setLiveStatus('ðŸ”Š Playing AI response...');
          const url = URL.createObjectURL(audioBlob);
          const responseAudio = new Audio(url);
          responseAudio.play();
          responseAudio.onended = () => {
            showVoiceState('listening');
            setLiveStatus('Ready for your next message. Speak naturally!');
          };
          return;
        }
        // Otherwise it's text: handle transcript or answer
        try {
          const msg = JSON.parse(event.data);
          if (msg.type === 'answer') {
            await pc.setRemoteDescription(new RTCSessionDescription(msg));
            showVoiceState('listening');
            setLiveStatus('ðŸŽ¤ Voice chat ready! Start speaking...');
          } else if (msg.transcript) {
            showVoiceState('processing');
            setLiveStatus('You said: "' + msg.transcript + '" - Getting AI response...');
          }
        } catch (e) {
          setLiveStatus('Received unknown message: ' + event.data);
        }
      };

      setLiveStatus('Negotiating WebRTCâ€¦');
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);
      ws.send(JSON.stringify({ type: 'offer', sdp: offer.sdp }));

      liveStart.disabled = true;
      liveStop.disabled = false;

    } catch (err) {
      setLiveStatus('WebRTC error: ' + err.message);
      if (ws) ws.close();
      if (pc) pc.close();
      liveStart.disabled = false;
      liveStop.disabled = true;
    }
  };

  liveStop.onclick = () => {
    if (ws) try { ws.close(); } catch {}
    if (pc) try { pc.close(); } catch {}
    if (liveStream) {
      liveStream.getTracks().forEach(t => t.stop());
    }
    showVoiceState('hidden');
    setLiveStatus('Voice chat stopped.');
    liveStart.disabled = false;
    liveStop.disabled = true;
  };

  </script>
</body>
</html>