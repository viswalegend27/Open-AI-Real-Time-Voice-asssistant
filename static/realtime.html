<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenAI Realtime Voice Assistant</title>
    <style>
        :root {
            --bg: #0f1115;
            --panel: #161a22;
            --text: #e6e9ef;
            --muted: #9aa4b2;
            --accent: #3b82f6;
            --success: #10b981;
            --warning: #f59e0b;
            --error: #ef4444;
            --border: #222837;
        }
        
        * { margin: 0; padding: 0; box-sizing: border-box; }
        
        html, body { 
            height: 100%; 
            font-family: ui-sans-serif, system-ui, -apple-system, 'Segoe UI', Roboto, Arial, sans-serif;
        }
        
        body {
            margin: 0;
            background: radial-gradient(1200px 800px at 10% -20%, #1a2030 0, #0f1115 60%) fixed;
            color: var(--text);
        }
        
        .container { 
            max-width: 1200px; 
            margin: 32px auto; 
            padding: 0 20px; 
        }
        
        h1 { 
            margin: 0 0 10px; 
            font-weight: 700; 
            letter-spacing: 0.2px; 
            font-size: 2em;
            background: linear-gradient(135deg, #3b82f6 0%, #8b5cf6 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .subtitle {
            color: var(--muted);
            margin-bottom: 24px;
            font-size: 1.1em;
        }
        
        #controls {
            display: flex; 
            align-items: center; 
            gap: 12px; 
            margin-bottom: 24px;
            flex-wrap: wrap;
        }
        
        #controls button {
            padding: 12px 24px; 
            border-radius: 10px; 
            border: 1px solid var(--border);
            background: linear-gradient(135deg, #1a2230 0%, #161a22 100%);
            color: var(--text); 
            cursor: pointer; 
            transition: all 0.2s ease;
            font-weight: 600;
            font-size: 15px;
        }
        
        #controls button:hover:not(:disabled) { 
            border-color: var(--accent); 
            box-shadow: 0 0 0 3px rgba(59,130,246,0.15) inset;
            transform: translateY(-1px);
        }
        
        #controls button:disabled { 
            opacity: 0.5; 
            cursor: not-allowed; 
        }
        
        #startBtn {
            background: linear-gradient(135deg, var(--success) 0%, #059669 100%);
            border-color: var(--success);
        }
        
        #stopBtn {
            background: linear-gradient(135deg, var(--error) 0%, #dc2626 100%);
            border-color: var(--error);
        }
        
        #status { 
            color: var(--muted); 
            font-size: 0.95em; 
            margin-left: 12px;
            padding: 8px 16px;
            background: rgba(255,255,255,0.05);
            border-radius: 8px;
            border: 1px solid var(--border);
        }
        
        .status-live {
            color: var(--success) !important;
            border-color: var(--success) !important;
            animation: pulse 2s infinite;
        }
        
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.7; }
        }
        
        .row { 
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 20px; 
            margin-bottom: 20px;
        }
        
        .panel {
            background: var(--panel); 
            border: 1px solid var(--border); 
            border-radius: 12px;
            padding: 20px; 
            min-height: 250px; 
            box-shadow: 0 8px 30px rgba(0,0,0,0.28);
            transition: all 0.3s ease;
        }
        
        .panel:hover {
            border-color: rgba(59,130,246,0.3);
            box-shadow: 0 12px 40px rgba(0,0,0,0.35);
        }
        
        .label { 
            font-weight: 600; 
            margin-bottom: 12px; 
            color: #b6c2d0; 
            font-size: 14px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }
        
        #userTranscript, #aiTranscript { 
            white-space: pre-wrap; 
            min-height: 180px; 
            line-height: 1.6;
            font-size: 15px;
        }
        
        .small { 
            color: var(--muted); 
            font-size: 12px; 
            margin-top: 12px; 
        }
        
        audio { 
            width: 100%; 
            margin-top: 12px; 
            outline: none;
            border-radius: 8px;
        }
        
        .muted { 
            color: var(--muted); 
            font-style: italic;
        }
        
        .info-box {
            background: rgba(59,130,246,0.1);
            border: 1px solid rgba(59,130,246,0.3);
            border-radius: 10px;
            padding: 16px;
            margin-top: 20px;
            color: var(--text);
        }
        
        .info-box strong {
            color: var(--accent);
        }
        
        .feature-list {
            list-style: none;
            margin-top: 12px;
        }
        
        .feature-list li {
            padding: 6px 0;
            padding-left: 24px;
            position: relative;
        }
        
        .feature-list li:before {
            content: "‚úì";
            position: absolute;
            left: 0;
            color: var(--success);
            font-weight: bold;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üé§ OpenAI Realtime Voice Assistant</h1>
        <p class="subtitle">Direct WebRTC connection to OpenAI - Ultra-low latency voice chat</p>
        
        <div id="controls">
            <button id="startBtn">üéôÔ∏è Start Conversation</button>
            <button id="stopBtn" disabled>‚èπÔ∏è Stop</button>
            <span id="status">Ready to connect</span>
        </div>

        <div class="row">
            <div class="panel">
                <div class="label">üë§ Your Speech (Live Transcription)</div>
                <div id="userTranscript" class="muted">Click "Start Conversation" and begin speaking...</div>
            </div>
            
            <div class="panel">
                <div class="label">ü§ñ AI Assistant</div>
                <div id="aiTranscript" class="muted">The assistant's responses will appear here.</div>
                <div class="small">Audio playback:</div>
                <audio id="aiAudio" autoplay playsinline></audio>
            </div>
        </div>

        <div class="info-box">
            <strong>üí° Features:</strong>
            <ul class="feature-list">
                <li>Direct browser-to-OpenAI WebRTC connection (no server audio processing)</li>
                <li>Ultra-low latency real-time conversation</li>
                <li>Automatic voice activity detection</li>
                <li>Live speech-to-text transcription</li>
                <li>Natural conversational AI responses</li>
            </ul>
            <div class="small" style="margin-top: 12px;">
                <strong>Note:</strong> Use Chrome/Edge on http://127.0.0.1:8000 for best results. 
                Ensure your OpenAI account has access to the Realtime API.
            </div>
        </div>
    </div>

    <script>
    (function() {
        // ========================================================================
        // DOM ELEMENTS & STATE
        // ========================================================================
        const startBtn = document.getElementById("startBtn");
        const stopBtn = document.getElementById("stopBtn");
        const statusEl = document.getElementById("status");
        const userEl = document.getElementById("userTranscript");
        const aiEl = document.getElementById("aiTranscript");
        const aiAudio = document.getElementById("aiAudio");

        let pc = null;
        let micStream = null;
        let eventsDC = null;
        const DEBUG_EVENTS = true; // Set to false in production

        // ========================================================================
        // UI HELPER FUNCTIONS
        // ========================================================================
        function setStatus(text, isLive = false) {
            statusEl.textContent = text;
            if (isLive) {
                statusEl.classList.add('status-live');
            } else {
                statusEl.classList.remove('status-live');
            }
        }

        function clearEl(el) {
            el.textContent = "";
            el.classList.remove("muted");
        }

        function append(el, text) {
            el.textContent += text;
            el.scrollTop = el.scrollHeight;
        }

        function appendLine(el, text) {
            append(el, (text || "") + "\n");
        }

        // ========================================================================
        // WEBRTC EVENT HANDLING
        // ========================================================================
        function attachEventChannel(dc) {
            if (!dc) return;
            eventsDC = dc;
            
            eventsDC.onopen = () => {
                console.log("‚úÖ Events channel opened");
                setStatus("üéôÔ∏è Connected - Speak now!", true);
            };
            
            eventsDC.onclose = () => {
                console.log("‚ùå Events channel closed");
                setStatus("Events channel closed");
            };
            
            eventsDC.onerror = (err) => {
                console.error("Events channel error:", err);
            };
            
            eventsDC.onmessage = (ev) => {
                let evt;
                try {
                    evt = JSON.parse(ev.data);
                } catch (e) {
                    console.error("Failed to parse event:", e);
                    return;
                }
                
                if (DEBUG_EVENTS) {
                    console.log("üì® OpenAI Event:", evt.type, evt);
                }
                
                handleRealtimeEvent(evt);
            };
        }

        function handleRealtimeEvent(evt) {
            const t = evt?.type || "";

            // Handle input transcription (user's speech)
            if (t.includes("conversation.item.input_audio_transcription")) {
                if (t.includes("completed")) {
                    const text = evt?.transcript || "";
                    if (text) {
                        appendLine(userEl, text);
                    }
                }
                return;
            }

            // Handle real-time transcript deltas
            if (t.includes("input_audio_transcription.delta")) {
                const text = extractText(evt);
                if (text) append(userEl, text);
                return;
            }

            if (t.includes("input_audio_transcription.completed")) {
                appendLine(userEl, "");
                return;
            }

            // Handle AI response text
            if (t === "response.text.delta") {
                const text = extractText(evt);
                if (text) append(aiEl, text);
                return;
            }

            if (t === "response.text.done") {
                appendLine(aiEl, "");
                return;
            }

            // Handle AI audio response
            if (t === "response.audio_transcript.delta") {
                const text = extractText(evt);
                if (text) append(aiEl, text);
                return;
            }

            if (t === "response.audio_transcript.done") {
                appendLine(aiEl, "");
                return;
            }

            // Generic response handling
            if (t.startsWith("response.") && t.endsWith(".delta")) {
                const text = extractText(evt);
                if (text) append(aiEl, text);
                return;
            }

            if (t === "response.done") {
                console.log("‚úÖ Response complete");
                return;
            }

            // Error handling
            if (t === "error") {
                console.error("OpenAI Error:", evt);
                appendLine(aiEl, `[Error: ${evt.error?.message || "Unknown error"}]`);
            }
        }

        function extractText(evt) {
            if (!evt || typeof evt !== "object") return null;
            if (typeof evt.delta === "string") return evt.delta;
            if (evt.delta && typeof evt.delta.text === "string") return evt.delta.text;
            if (typeof evt.text === "string") return evt.text;
            if (typeof evt.transcript === "string") return evt.transcript;
            if (typeof evt.message === "string") return evt.message;
            if (evt.output_text && Array.isArray(evt.output_text)) {
                return evt.output_text.join("");
            }
            return null;
        }

        // ========================================================================
        // MAIN APPLICATION LOGIC
        // ========================================================================
        async function start() {
            try {
                // 1. Initialize UI
                startBtn.disabled = true;
                stopBtn.disabled = false;
                setStatus("Requesting microphone access...");
                clearEl(userEl);
                clearEl(aiEl);

                // 2. Get microphone access
                try {
                    micStream = await navigator.mediaDevices.getUserMedia({ 
                        audio: {
                            echoCancellation: true,
                            noiseSuppression: true,
                            autoGainControl: true
                        }
                    });
                    console.log("‚úÖ Microphone access granted");
                } catch (e) {
                    console.error("Microphone error:", e);
                    setStatus("‚ùå Microphone access denied");
                    startBtn.disabled = false;
                    stopBtn.disabled = true;
                    alert("Please allow microphone access to use this app.");
                    return;
                }

                // 3. Create session with backend
                setStatus("Creating OpenAI session...");
                let session;
                try {
                    const response = await fetch("/session/");
                    if (!response.ok) {
                        const errorText = await response.text();
                        throw new Error(errorText);
                    }
                    session = await response.json();
                    console.log("‚úÖ Session created:", session);
                } catch (e) {
                    console.error("Session creation error:", e);
                    setStatus("‚ùå Failed to create session");
                    startBtn.disabled = false;
                    stopBtn.disabled = true;
                    alert(`Failed to create session: ${e.message}`);
                    return;
                }

                // 4. Extract session details
                const ephemeralKey = session?.client_secret?.value;
                const model = session?.model || "gpt-4o-realtime-preview-2024-12-17";
                
                if (!ephemeralKey) {
                    console.error("Invalid session response:", session);
                    setStatus("‚ùå Invalid session");
                    startBtn.disabled = false;
                    stopBtn.disabled = true;
                    alert("Invalid session response from server");
                    return;
                }

                // 5. Initialize WebRTC
                setStatus("Setting up WebRTC connection...");
                pc = new RTCPeerConnection({
                    iceServers: [{ urls: ["stun:stun.l.google.com:19302"] }]
                });

                // 6. Handle data channel for events
                pc.ondatachannel = (event) => {
                    if (event.channel?.label === "oai-events") {
                        console.log("üì° Received data channel:", event.channel.label);
                        attachEventChannel(event.channel);
                    }
                };

                // Proactively create data channel
                const proactiveDC = pc.createDataChannel("oai-events");
                attachEventChannel(proactiveDC);

                // 7. Handle incoming audio track
                pc.ontrack = (event) => {
                    console.log("üéµ Received audio track");
                    const [stream] = event.streams;
                    aiAudio.srcObject = stream;
                };

                // 8. Add local microphone track
                for (const track of micStream.getTracks()) {
                    pc.addTrack(track, micStream);
                }

                // 9. Add transceiver for receiving audio
                pc.addTransceiver("audio", { direction: "recvonly" });

                // 10. Create and send offer
                const offer = await pc.createOffer();
                await pc.setLocalDescription(offer);

                console.log("üì§ Sending SDP offer to OpenAI...");
                setStatus("Connecting to OpenAI...");

                // 11. Exchange SDP with OpenAI
                let answerSdp = "";
                try {
                    const sdpResponse = await fetch(
                        `https://api.openai.com/v1/realtime?model=${encodeURIComponent(model)}`,
                        {
                            method: "POST",
                            headers: {
                                "Authorization": `Bearer ${ephemeralKey}`,
                                "Content-Type": "application/sdp",
                                "OpenAI-Beta": "realtime=v1",
                            },
                            body: offer.sdp,
                        }
                    );

                    if (!sdpResponse.ok) {
                        const errorText = await sdpResponse.text();
                        throw new Error(errorText);
                    }

                    answerSdp = await sdpResponse.text();
                    console.log("‚úÖ Received SDP answer from OpenAI");
                } catch (e) {
                    console.error("SDP exchange error:", e);
                    setStatus("‚ùå OpenAI connection failed");
                    stop();
                    alert(`Failed to connect to OpenAI: ${e.message}`);
                    return;
                }

                // 12. Set remote description
                await pc.setRemoteDescription({ type: "answer", sdp: answerSdp });
                console.log("‚úÖ WebRTC connection established");
                setStatus("üéôÔ∏è Connected - Speak now!", true);

            } catch (error) {
                console.error("Start error:", error);
                setStatus("‚ùå Connection failed");
                stop();
                alert(`Error: ${error.message}`);
            }
        }

        function stop() {
            console.log("üõë Stopping session...");
            stopBtn.disabled = true;
            startBtn.disabled = false;
            setStatus("Disconnecting...");

            try {
                if (eventsDC) {
                    eventsDC.close();
                    eventsDC = null;
                }
            } catch (e) {
                console.error("Error closing data channel:", e);
            }

            try {
                if (pc) {
                    pc.getSenders().forEach(sender => {
                        try {
                            if (sender.track) sender.track.stop();
                        } catch (e) {
                            console.error("Error stopping sender track:", e);
                        }
                    });
                    
                    pc.getReceivers().forEach(receiver => {
                        try {
                            if (receiver.track) receiver.track.stop();
                        } catch (e) {
                            console.error("Error stopping receiver track:", e);
                        }
                    });
                    
                    pc.close();
                    pc = null;
                }
            } catch (e) {
                console.error("Error closing peer connection:", e);
            }

            if (micStream) {
                micStream.getTracks().forEach(track => track.stop());
                micStream = null;
            }

            setStatus("Ready to connect");
            console.log("‚úÖ Session stopped");
        }

        // ========================================================================
        // EVENT LISTENERS
        // ========================================================================
        startBtn.addEventListener("click", start);
        stopBtn.addEventListener("click", stop);

        // Cleanup on page unload
        window.addEventListener("beforeunload", stop);

        console.log("üé§ OpenAI Realtime Voice Assistant loaded");
    })();
    </script>
</body>
</html>
