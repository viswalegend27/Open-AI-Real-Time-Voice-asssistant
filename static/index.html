<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Assistant</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
            padding: 20px;
        }
        .container {
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            max-width: 600px;
            width: 100%;
            padding: 40px;
        }
        h1 {
            color: #667eea;
            text-align: center;
            margin-bottom: 10px;
            font-size: 32px;
        }
        .subtitle {
            text-align: center;
            color: #666;
            margin-bottom: 30px;
            font-size: 14px;
        }
        .status {
            background: #f7fafc;
            border-radius: 10px;
            padding: 20px;
            margin-bottom: 20px;
            min-height: 100px;
        }
        .status-text {
            color: #4a5568;
            line-height: 1.6;
        }
        .transcript {
            background: #e6fffa;
            border-left: 4px solid #38b2ac;
            padding: 15px;
            margin: 10px 0;
            border-radius: 5px;
        }
        .response {
            background: #f0fff4;
            border-left: 4px solid #48bb78;
            padding: 15px;
            margin: 10px 0;
            border-radius: 5px;
        }
        .error {
            background: #fff5f5;
            border-left: 4px solid #f56565;
            padding: 15px;
            margin: 10px 0;
            border-radius: 5px;
            color: #c53030;
        }
        .controls {
            display: flex;
            gap: 15px;
            justify-content: center;
            flex-wrap: wrap;
        }
        button {
            background: #667eea;
            color: white;
            border: none;
            padding: 15px 30px;
            font-size: 16px;
            border-radius: 10px;
            cursor: pointer;
            transition: all 0.3s;
            font-weight: 600;
        }
        button:hover:not(:disabled) {
            background: #5568d3;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
        }
        button:disabled {
            background: #cbd5e0;
            cursor: not-allowed;
        }
        .recording {
            background: #f56565 !important;
            animation: pulse 1.5s infinite;
        }
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.7; }
        }
        .instructions {
            background: #edf2f7;
            border-radius: 10px;
            padding: 15px;
            margin-top: 20px;
            font-size: 13px;
            color: #4a5568;
        }
        .instructions h3 {
            color: #2d3748;
            margin-bottom: 10px;
            font-size: 14px;
        }
        .instructions ul {
            margin-left: 20px;
        }
        .instructions li {
            margin: 5px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ðŸŽ¤ Voice Assistant</h1>
        <p class="subtitle">Powered by OpenAI - Simple & Reliable</p>
        
        <div class="status">
            <div class="status-text" id="status">
                Click "Start Recording" to begin speaking to your assistant
            </div>
        </div>
        
        <div class="controls">
            <button id="startBtn">Start Recording</button>
            <button id="stopBtn" disabled>Stop Recording</button>
        </div>
        
        <div class="instructions">
            <h3>ðŸ“‹ How to Use:</h3>
            <ul>
                <li>Click <strong>"Start Recording"</strong></li>
                <li>Speak clearly for at least 2 seconds</li>
                <li>Click <strong>"Stop Recording"</strong> when done</li>
                <li>Wait for the AI response!</li>
            </ul>
        </div>
    </div>
    
    <script>
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const statusDiv = document.getElementById('status');
        
        let ws = null;
        let mediaRecorder = null;
        let audioContext = null;
        let audioChunks = [];
        
        function setStatus(html, type = 'info') {
            const className = type === 'error' ? 'error' : type === 'transcript' ? 'transcript' : type === 'response' ? 'response' : '';
            const div = document.createElement('div');
            div.className = className;
            div.innerHTML = html;
            statusDiv.appendChild(div);
            statusDiv.scrollTop = statusDiv.scrollHeight;
        }
        
        function clearStatus() {
            statusDiv.innerHTML = 'Ready to record...';
        }
        
        startBtn.onclick = async () => {
            try {
                clearStatus();
                setStatus('ðŸ”Œ Connecting to server...');
                
                // Connect WebSocket
                ws = new WebSocket(`ws://${window.location.host}/ws/voice`);
                
                ws.onopen = async () => {
                    setStatus('âœ… Connected! Starting microphone...');
                    
                    // Get microphone access
                    const stream = await navigator.mediaDevices.getUserMedia({
                        audio: {
                            echoCancellation: true,
                            noiseSuppression: true,
                            autoGainControl: true,
                            channelCount: 1,
                            sampleRate: 16000
                        }
                    });
                    
                    // Create AudioContext for processing
                    audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
                    const source = audioContext.createMediaStreamSource(stream);
                    const processor = audioContext.createScriptProcessor(4096, 1, 1);
                    
                    source.connect(processor);
                    processor.connect(audioContext.destination);
                    
                    audioChunks = [];
                    
                    processor.onaudioprocess = (e) => {
                        // Get audio data as Float32
                        const inputData = e.inputBuffer.getChannelData(0);
                        // Convert to Int16 for transmission
                        const int16Data = new Int16Array(inputData.length);
                        for (let i = 0; i < inputData.length; i++) {
                            const s = Math.max(-1, Math.min(1, inputData[i]));
                            int16Data[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                        }
                        audioChunks.push(int16Data);
                        
                        // Send chunk to server
                        if (ws && ws.readyState === WebSocket.OPEN) {
                            const base64 = btoa(String.fromCharCode.apply(null, new Uint8Array(int16Data.buffer)));
                            ws.send(JSON.stringify({
                                type: 'audio_chunk',
                                data: base64
                            }));
                        }
                    };
                    
                    // Store for cleanup
                    window.currentStream = stream;
                    window.currentProcessor = processor;
                    
                    setStatus('ðŸŽ™ï¸ <strong>Recording... Speak now!</strong>');
                    startBtn.disabled = true;
                    stopBtn.disabled = false;
                    startBtn.classList.remove('recording');
                    stopBtn.classList.add('recording');
                };
                
                ws.onmessage = (event) => {
                    const msg = JSON.parse(event.data);
                    
                    if (msg.type === 'transcript') {
                        setStatus(`<strong>You said:</strong> "${msg.text}"`, 'transcript');
                        setStatus('ðŸ¤– AI is thinking...');
                    }
                    else if (msg.type === 'audio') {
                        setStatus(`<strong>AI replied:</strong> ${msg.text}`, 'response');
                        
                        // Play audio response
                        const audioData = atob(msg.data);
                        const audioArray = new Uint8Array(audioData.length);
                        for (let i = 0; i < audioData.length; i++) {
                            audioArray[i] = audioData.charCodeAt(i);
                        }
                        const blob = new Blob([audioArray], { type: 'audio/mpeg' });
                        const url = URL.createObjectURL(blob);
                        const audio = new Audio(url);
                        audio.play();
                        
                        setStatus('âœ… Ready for next question!');
                    }
                    else if (msg.type === 'error') {
                        setStatus(`âš ï¸ ${msg.message}`, 'error');
                    }
                };
                
                ws.onerror = (error) => {
                    setStatus('âŒ Connection error. Please refresh and try again.', 'error');
                    cleanup();
                };
                
                ws.onclose = () => {
                    console.log('WebSocket closed');
                    cleanup();
                };
                
            } catch (err) {
                setStatus(`âŒ Error: ${err.message}`, 'error');
                console.error(err);
                cleanup();
            }
        };
        
        stopBtn.onclick = () => {
            if (ws && ws.readyState === WebSocket.OPEN) {
                setStatus('â¹ï¸ Stopping... Processing your audio...');
                ws.send(JSON.stringify({ type: 'audio_end' }));
            }
            cleanup();
        };
        
        function cleanup() {
            if (window.currentProcessor) {
                window.currentProcessor.disconnect();
                window.currentProcessor = null;
            }
            if (window.currentStream) {
                window.currentStream.getTracks().forEach(track => track.stop());
                window.currentStream = null;
            }
            if (audioContext && audioContext.state !== 'closed') {
                audioContext.close();
                audioContext = null;
            }
            
            startBtn.disabled = false;
            stopBtn.disabled = true;
            startBtn.classList.remove('recording');
            stopBtn.classList.remove('recording');
        }
    </script>
</body>
</html>
