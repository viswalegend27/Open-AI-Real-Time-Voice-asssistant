<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Voice-Assistant with OpenAI</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <style>
    body { font-family: system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif; padding: 20px; line-height: 1.4; }
    button { padding: 10px 16px; font-size: 16px; margin-right: 8px; }
    #status { margin-top: 10px; white-space: pre-wrap; }
    .muted { color: #666; font-size: 14px; }
  </style>
</head>
<body>
  <h2>Voice to Voice Assistant</h2>
  <p class="muted">Records mic → POST <code>/chat</code> → plays MP3. No calls to <code>/v1/*</code>.</p>

  <button id="record">Start Recording</button>
  <button id="stop" disabled>Stop</button>
  <div id="status"></div>
  <audio id="response-audio" controls></audio>

  <hr />
  <h3>Live Stream via WebRTC (Experimental)</h3>
  <button id="live-start">Start Live WebRTC</button>
  <button id="live-stop" disabled>Stop Live</button>
  <div id="live-status" class="muted"></div>

  <script>
    // --- Guard: scream if anything tries to call /v1/* from this page ---
    const _origFetch = window.fetch;
    window.fetch = async (input, init) => {
      const url = typeof input === 'string' ? input : input.url;
      if (url && url.startsWith('/v1/')) {
        console.error('Blocked a /v1/* call from this page:', url);
        throw new Error('This page must not call ' + url);
      }
      return _origFetch(input, init);
    };

    let recorder, audioStream;
    let chunks = [];
    const recordBtn = document.getElementById('record');
    const stopBtn = document.getElementById('stop');
    const statusDiv = document.getElementById('status');
    const responseAudio = document.getElementById('response-audio');

    function setStatus(msg) {
      statusDiv.textContent = msg;
      console.log(msg);
    }

    async function getRecorder() {
      if (!navigator.mediaDevices?.getUserMedia) {
        throw new Error('getUserMedia not supported in this browser.');
      }
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

      // Prefer an explicit mime type if supported
      const preferred = 'audio/webm;codecs=opus';
      const type = MediaRecorder.isTypeSupported?.(preferred) ? preferred : 'audio/webm';
      const rec = new MediaRecorder(stream, { mimeType: type });

      return { rec, stream, type };
    }

    recordBtn.onclick = async () => {
      try {
        const { rec, stream, type } = await getRecorder();
        recorder = rec;
        audioStream = stream;
        chunks = [];
        setStatus('Recording... click "Stop" when done.\nFormat: ' + type);

        recorder.ondataavailable = (e) => { if (e.data && e.data.size) chunks.push(e.data); };
        recorder.onstop = async () => {
          try {
            setStatus('Processing...');
            const blob = new Blob(chunks, { type });
            chunks = [];

            const form = new FormData();
            // Backend accepts webm/opus just fine; filename helps model inference.
            form.append('file', blob, 'input.webm');

            const res = await fetch('/chat', { method: 'POST', body: form });

            if (!res.ok) {
              // Try to read error detail from FastAPI
              let detail = '';
              try { detail = await res.text(); } catch {}
              throw new Error('Server error ' + res.status + (detail ? (': ' + detail) : ''));
            }

            const buf = await res.arrayBuffer();   // streaming → full blob is fine for now
            const outBlob = new Blob([buf], { type: 'audio/mpeg' });
            const url = URL.createObjectURL(outBlob);

            responseAudio.src = url;
            await responseAudio.play().catch(() => {/* autoplay may be blocked */});
            setStatus('Response ready!');

          } catch (err) {
            console.error(err);
            setStatus('Error: ' + err.message);
          }
        };

        recorder.start(); // start chunks
        recordBtn.disabled = true;
        stopBtn.disabled = false;

      } catch (err) {
        console.error(err);
        setStatus('Mic error: ' + err.message);
      }
    };

    stopBtn.onclick = () => {
      try {
        if (recorder && recorder.state !== 'inactive') {
          recorder.stop();
        }
        if (audioStream) {
          audioStream.getTracks().forEach(t => t.stop());
        }
      } finally {
        recordBtn.disabled = false;
        stopBtn.disabled = true;
      }
    };
  // WebRTC Live Audio Section
  const liveStart = document.getElementById('live-start');
  const liveStop = document.getElementById('live-stop');
  const liveStatus = document.getElementById('live-status');
  let liveStream = null;
  let pc = null;
  let ws = null;

  function setLiveStatus(msg) {
    liveStatus.textContent = msg;
    console.log('[LIVE]', msg);
  }

  liveStart.onclick = async () => {
    try {
      pc = new RTCPeerConnection();
      ws = new WebSocket(
        (location.protocol === 'https:' ? 'wss://' : 'ws://') + location.host + '/ws'
      );
      setLiveStatus('Connecting WebSocket for signaling…');
      await new Promise((resolve, reject) => {
        ws.onopen = resolve;
        ws.onerror = () => reject(new Error('WebSocket failed to open'));
      });

      setLiveStatus('Getting microphone…');
      liveStream = await navigator.mediaDevices.getUserMedia({ audio: true });
      liveStream.getTracks().forEach(track => pc.addTrack(track, liveStream));

      pc.onicecandidate = (ev) => {
        if (ev.candidate && ws && ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify({ type: 'candidate', candidate: ev.candidate }));
        }
      };

      ws.onmessage = async (event) => {
        let audioBlob = null;
        // Try to handle binary audio data in the most robust way
        if (event.data instanceof Blob) {
          audioBlob = event.data;
        } else if (event.data instanceof ArrayBuffer) {
          audioBlob = new Blob([event.data], { type: 'audio/mpeg' });
        } else if (event.data && event.data.arrayBuffer) {
          // For some browsers, event.data may look like a Blob but not evaluate instanceof
          try {
            const buffer = await event.data.arrayBuffer();
            audioBlob = new Blob([buffer], { type: 'audio/mpeg' });
          } catch {}
        }

        if (audioBlob) {
          setLiveStatus('Received reply audio, playing…');
          const url = URL.createObjectURL(audioBlob);
          const responseAudio = new Audio(url);
          responseAudio.play();
          return;
        }
        // Otherwise it's text: handle transcript or answer
        try {
          const msg = JSON.parse(event.data);
          if (msg.type === 'answer') {
            await pc.setRemoteDescription(new RTCSessionDescription(msg));
            setLiveStatus('WebRTC connection established! Streaming mic audio.');
          } else if (msg.transcript) {
            setLiveStatus('You said: ' + msg.transcript + '\nAwaiting voice reply…');
          }
        } catch (e) {
          setLiveStatus('Received unknown message: ' + event.data);
        }
      };

      setLiveStatus('Negotiating WebRTC…');
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);
      ws.send(JSON.stringify({ type: 'offer', sdp: offer.sdp }));

      liveStart.disabled = true;
      liveStop.disabled = false;

    } catch (err) {
      setLiveStatus('WebRTC error: ' + err.message);
      if (ws) ws.close();
      if (pc) pc.close();
      liveStart.disabled = false;
      liveStop.disabled = true;
    }
  };

  liveStop.onclick = () => {
    if (ws) try { ws.close(); } catch {}
    if (pc) try { pc.close(); } catch {}
    if (liveStream) {
      liveStream.getTracks().forEach(t => t.stop());
    }
    setLiveStatus('Stopped.');
    liveStart.disabled = false;
    liveStop.disabled = true;
  };

  </script>
</body>
</html>